{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Mirco StyleGAN**"
      ],
      "metadata": {
        "id": "Nbi7GwlH6JnJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S-yDIcy5SNF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def show_tensor_images(image_tensor, num_images=16, size=(3, 64, 64), nrow=3):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images,\n",
        "    size per image, and images per row, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu().clamp_(0, 1)\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow, padding=0)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Truncation Trick**\n",
        "\n",
        "The truncation trick resamples the noise vector z from a truncated normal distribution which allows you to tune the generator's fidelity/diversity. The truncation value is at least 0, where 1 means there is little truncation (high diversity) and 0 means the distribution is all truncated except for the mean (high quality/fidelity)"
      ],
      "metadata": {
        "id": "4z-RTcND6SVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import truncnorm\n",
        "def get_truncated_noise(n_samples, z_dim, truncation):\n",
        "    '''\n",
        "    Function for creating truncated noise vectors: Given the dimensions (n_samples, z_dim)\n",
        "    and truncation value, creates a tensor of that shape filled with random\n",
        "    numbers from the truncated normal distribution.\n",
        "    Parameters:\n",
        "        n_samples: the number of samples to generate, a scalar\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        truncation: the truncation value, a non-negative scalar\n",
        "    '''\n",
        "    truncated_noise = truncnorm.rvs(-1*truncation, truncation, size=(n_samples, z_dim))\n",
        "    return torch.Tensor(truncated_noise)"
      ],
      "metadata": {
        "id": "lhhfXZlM6pth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping z to w**\n",
        "\n",
        "z can be represented in a more disentangled space which makes the features easier to control later.\n",
        "\n",
        "The mapping network in StyleGAN is composed of 8 layers, but for this implementation, we will use a neural network with 3 layers"
      ],
      "metadata": {
        "id": "8PdCMm2m78ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MappingLayers(nn.Module):\n",
        "    '''\n",
        "    Mapping Layers Class\n",
        "    Values:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
        "    '''\n",
        " \n",
        "    def __init__(self, z_dim, hidden_dim, w_dim):\n",
        "        super().__init__()\n",
        "        self.mapping = nn.Sequential(\n",
        "            nn.Linear(z_dim,hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim,hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, w_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, noise):\n",
        "        '''\n",
        "        Function for completing a forward pass of MappingLayers: \n",
        "        Given an initial noise tensor, returns the intermediate noise tensor.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "        '''\n",
        "        return self.mapping(noise)"
      ],
      "metadata": {
        "id": "2nRA5gue70M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Noise Injection**\n",
        "\n",
        "The noise tensor is not entirely random; it is initialized as one random channel that is then multiplied by learned weights for each channel in the image."
      ],
      "metadata": {
        "id": "yw7w243a_X03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InjectNoise(nn.Module):\n",
        "    '''\n",
        "    Inject Noise Class\n",
        "    Values:\n",
        "        channels: the number of channels the image has, a scalar\n",
        "    '''\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter( # You use nn.Parameter so that these weights can be optimized\n",
        "            # Initiate the weights for the channels from a random normal distribution\n",
        "            torch.randn(channels)[None, :, None, None] #torch.randn((1,channels,1,1))\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        '''\n",
        "        Function for completing a forward pass of InjectNoise: Given an image, \n",
        "        returns the image with random noise added.\n",
        "        Parameters:\n",
        "            image: the feature map of shape (n_samples, channels, width, height)\n",
        "        '''\n",
        "        noise_shape = (image.shape[0], 1, image.shape[2], image.shape[3])\n",
        "        \n",
        "        noise = torch.randn(noise_shape, device=image.device) # Creates the random noise\n",
        "        return image + self.weight * noise # Applies to image after multiplying by the weight for each channel"
      ],
      "metadata": {
        "id": "_fBpnaiw_gUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adaptive Instance Normalization (AdaIN)**\n",
        "\n",
        "To increase control over the image, you inject w — the intermediate noise vector — multiple times throughout StyleGAN. This is done by transforming it into a set of style parameters and introducing the style to the image through AdaIN."
      ],
      "metadata": {
        "id": "VpBYVVaBBVOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaIN(nn.Module):\n",
        "    '''\n",
        "    AdaIN Class\n",
        "    Values:\n",
        "        channels: the number of channels the image has, a scalar\n",
        "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, w_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Normalize the input per-dimension\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
        "\n",
        "        # You want to map w to a set of style weights per channel.\n",
        "        # Replace the Nones with the correct dimensions - keep in mind that \n",
        "        # both linear maps transform a w vector into style weights \n",
        "        # corresponding to the number of image channels.\n",
        "\n",
        "        self.style_scale_transform = nn.Linear(w_dim, channels)\n",
        "        self.style_shift_transform = nn.Linear(w_dim, channels)\n",
        "\n",
        "\n",
        "    def forward(self, image, w):\n",
        "        '''\n",
        "        Function for completing a forward pass of AdaIN: Given an image and intermediate noise vector w, \n",
        "        returns the normalized image that has been scaled and shifted by the style.\n",
        "        Parameters:\n",
        "            image: the feature map of shape (n_samples, channels, width, height)\n",
        "            w: the intermediate noise vector\n",
        "        '''\n",
        "        normalized_image = self.instance_norm(image)\n",
        "        style_scale = self.style_scale_transform(w)[:, :, None, None]\n",
        "        style_shift = self.style_shift_transform(w)[:, :, None, None]\n",
        "        \n",
        "        # Calculate the transformed image\n",
        "        transformed_image = style_scale * normalized_image + style_shift\n",
        "        return transformed_image"
      ],
      "metadata": {
        "id": "smvUcLRUCWW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Progressive Growing**\n",
        "\n",
        "This helps StyleGAN to create high resolution images by gradually doubling the image's size until the desired size."
      ],
      "metadata": {
        "id": "w5TMEyZJCnhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MicroStyleGANGeneratorBlock(nn.Module):\n",
        "    '''\n",
        "    Micro StyleGAN Generator Block Class\n",
        "    Values:\n",
        "        in_chan: the number of channels in the input, a scalar\n",
        "        out_chan: the number of channels wanted in the output, a scalar\n",
        "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
        "        kernel_size: the size of the convolving kernel\n",
        "        starting_size: the size of the starting image\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_chan, out_chan, w_dim, kernel_size, starting_size, use_upsample=True):\n",
        "        super().__init__()\n",
        "        self.use_upsample = use_upsample\n",
        "        # Replace the Nones in order to:\n",
        "        # 1. Upsample to the starting_size, bilinearly (https://pytorch.org/docs/master/generated/torch.nn.Upsample.html)\n",
        "        # 2. Create a kernel_size convolution which takes in \n",
        "        #    an image with in_chan and outputs one with out_chan (https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "        # 3. Create an object to inject noise\n",
        "        # 4. Create an AdaIN object\n",
        "        # 5. Create a LeakyReLU activation with slope 0.2\n",
        "        \n",
        "\n",
        "        if self.use_upsample:\n",
        "            self.upsample = nn.Upsample((starting_size), mode='bilinear')\n",
        "        self.conv = nn.Conv2d(in_chan, out_chan, kernel_size, padding=1) # Padding is used to maintain the image size\n",
        "        self.inject_noise = InjectNoise(out_chan)\n",
        "        self.adain = AdaIN(out_chan, w_dim)\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        '''\n",
        "        Function for completing a forward pass of MicroStyleGANGeneratorBlock: Given an x and w, \n",
        "        computes a StyleGAN generator block.\n",
        "        Parameters:\n",
        "            x: the input into the generator, feature map of shape (n_samples, channels, width, height)\n",
        "            w: the intermediate noise vector\n",
        "        '''\n",
        "        if self.use_upsample:\n",
        "            x = self.upsample(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.inject_noise(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.adain(x, w)\n",
        "        return x"
      ],
      "metadata": {
        "id": "trXMiXEuCrOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MicroStyleGANGenerator(nn.Module):\n",
        "    '''\n",
        "    Micro StyleGAN Generator Class\n",
        "    Values:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        map_hidden_dim: the mapping inner dimension, a scalar\n",
        "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
        "        in_chan: the dimension of the constant input, usually w_dim, a scalar\n",
        "        out_chan: the number of channels wanted in the output, a scalar\n",
        "        kernel_size: the size of the convolving kernel\n",
        "        hidden_chan: the inner dimension, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, \n",
        "                 z_dim, \n",
        "                 map_hidden_dim,\n",
        "                 w_dim,\n",
        "                 in_chan,\n",
        "                 out_chan, \n",
        "                 kernel_size, \n",
        "                 hidden_chan):\n",
        "        super().__init__()\n",
        "        self.map = MappingLayers(z_dim, map_hidden_dim, w_dim)\n",
        "        # Typically this constant is initiated to all ones, but you will initiate to a\n",
        "        # Gaussian to better visualize the network's effect\n",
        "        self.starting_constant = nn.Parameter(torch.randn(1, in_chan, 4, 4))\n",
        "        self.block0 = MicroStyleGANGeneratorBlock(in_chan, hidden_chan, w_dim, kernel_size, 4, use_upsample=False)\n",
        "        self.block1 = MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 8)\n",
        "        self.block2 = MicroStyleGANGeneratorBlock(hidden_chan, hidden_chan, w_dim, kernel_size, 16)\n",
        "        # You need to have a way of mapping from the output noise to an image, \n",
        "        # so you learn a 1x1 convolution to transform the e.g. 512 channels into 3 channels\n",
        "        # (Note that this is simplified, with clipping used in the real StyleGAN)\n",
        "        self.block1_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=1)\n",
        "        self.block2_to_image = nn.Conv2d(hidden_chan, out_chan, kernel_size=1)\n",
        "        self.alpha = 0.2\n",
        "\n",
        "    def upsample_to_match_size(self, smaller_image, bigger_image):\n",
        "        '''\n",
        "        Function for upsampling an image to the size of another: Given a two images (smaller and bigger), \n",
        "        upsamples the first to have the same dimensions as the second.\n",
        "        Parameters:\n",
        "            smaller_image: the smaller image to upsample\n",
        "            bigger_image: the bigger image whose dimensions will be upsampled to\n",
        "        '''\n",
        "        return F.interpolate(smaller_image, size=bigger_image.shape[-2:], mode='bilinear')\n",
        "\n",
        "    def forward(self, noise, return_intermediate=False):\n",
        "        '''\n",
        "        Function for completing a forward pass of MicroStyleGANGenerator: Given noise, \n",
        "        computes a StyleGAN iteration.\n",
        "        Parameters:\n",
        "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
        "            return_intermediate: a boolean, true to return the images as well (for testing) and false otherwise\n",
        "        '''\n",
        "        x = self.starting_constant\n",
        "        w = self.map(noise)\n",
        "        x = self.block0(x, w)\n",
        "        x_small = self.block1(x, w) # First generator run output\n",
        "        x_small_image = self.block1_to_image(x_small)\n",
        "        x_big = self.block2(x_small, w) # Second generator run output \n",
        "        x_big_image = self.block2_to_image(x_big)\n",
        "        x_small_upsample = self.upsample_to_match_size(x_small_image, x_big_image) # Upsample first generator run output to be same size as second generator run output \n",
        "        # Interpolate between the upsampled image and the image from the generator using alpha\n",
        "        \n",
        "\n",
        "        interpolation = self.alpha * (x_big_image) + (1-self.alpha) * (x_small_upsample)\n",
        "\n",
        "        \n",
        "        if return_intermediate:\n",
        "            return interpolation, x_small_upsample, x_big_image\n",
        "        return interpolation"
      ],
      "metadata": {
        "id": "hCDAhXhyD7er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running**"
      ],
      "metadata": {
        "id": "BEnh2RjUEM2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_dim = 128\n",
        "out_chan = 3\n",
        "truncation = 0.7\n",
        "\n",
        "mu_stylegan = MicroStyleGANGenerator(\n",
        "    z_dim=z_dim, \n",
        "    map_hidden_dim=1024,\n",
        "    w_dim=496,\n",
        "    in_chan=512,\n",
        "    out_chan=out_chan, \n",
        "    kernel_size=3, \n",
        "    hidden_chan=256\n",
        ")\n",
        "\n",
        "test_samples = 10\n",
        "test_result = mu_stylegan(get_truncated_noise(test_samples, z_dim, truncation))\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [15, 15]\n",
        "\n",
        "viz_samples = 10\n",
        "# The noise is exaggerated for visual effect\n",
        "viz_noise = get_truncated_noise(viz_samples, z_dim, truncation) * 10\n",
        "\n",
        "mu_stylegan.eval()\n",
        "images = []\n",
        "for alpha in np.linspace(0, 1, num=5):\n",
        "    mu_stylegan.alpha = alpha\n",
        "    viz_result, _, _ =  mu_stylegan(\n",
        "        viz_noise, \n",
        "        return_intermediate=True)\n",
        "    images += [tensor for tensor in viz_result]\n",
        "show_tensor_images(torch.stack(images), nrow=viz_samples, num_images=len(images))\n",
        "mu_stylegan = mu_stylegan.train()"
      ],
      "metadata": {
        "id": "OXxKWZbVEPJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}